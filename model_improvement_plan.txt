MODEL IMPROVEMENT SUMMARY (LAPTOP-FRIENDLY VERSION) Project: Hybrid
BERT + Metadata Bot Detection (Cresci-2017)

================================================== SECTION 1 — CURRENT
STATUS (WHAT YOU HAVE ACHIEVED)
==================================================

Architecture: - Hybrid BERT + Metadata MLP model implemented - BERT used
as text feature extractor (Currently Frozen) - Metadata fused using MLP

Data Engineering: - Successfully processed large node.json (800MB) using
streaming (ijson) - Fixed ID mismatches between files - Handled boolean
NaN issue in ‘verified’ column - Created clean processed_data.csv

Training Stability: - Fixed NaN loss issue - Added gradient clipping -
Added weight decay regularization - Added metadata NaN safety check
(torch.nan_to_num)

Evaluation Performance: - Test Accuracy: ~0.76 - Test F1 Score: ~0.86 -
Model training stable across epochs

Strength Level: - Strong baseline implementation - Good debugging and
data engineering skills - Correct handling of class imbalance basics

================================================== SECTION 2 — CURRENT
LIMITATIONS ==================================================

1.  BERT is Frozen
    -   Limits learning of dataset-specific patterns
    -   Major reason performance is lower than research papers
2.  Basic Loss Function
    -   Using CrossEntropy with class weights
    -   Less effective than Focal Loss for imbalance
3.  Basic Optimizer Setup
    -   Using Adam without scheduler
    -   Missing LR warmup and cosine decay
4.  Normal Batch Sampling
    -   Not using stratified or weighted mini-batches
5.  Basic Metadata Features
    -   Using limited derived behavioral signals

================================================== SECTION 3 — HIGH
PRIORITY IMPROVEMENTS (BEST ROI)
==================================================

PRIORITY 1 — Partial BERT Fine-Tuning

Unfreeze last 1–2 transformer layers only.

Expected Gain: +0.03 to +0.06 F1

Why: Allows BERT to adapt to Twitter bot language patterns.

Laptop Tip: Freeze most layers to reduce GPU/CPU load.

------------------------------------------------------------------------

PRIORITY 2 — Replace CrossEntropy with Focal Loss

Improves minority class learning.

Expected Gain: +0.02 to +0.04 F1

Why: Better handles bot vs human imbalance.

------------------------------------------------------------------------

PRIORITY 3 — Stratified Mini-Batch Sampling

Ensures balanced class distribution per batch.

Expected Gain: +0.01 to +0.02 F1

Why: Prevents model bias toward majority class.

================================================== SECTION 4 — MEDIUM
PRIORITY IMPROVEMENTS ==================================================

1.  Switch Optimizer → AdamW
2.  Add Learning Rate Scheduler (Cosine / Step)
3.  Tune Dropout Values
4.  Try Batch Size Experiments (8 / 16 / 32)

Expected Gain Combined: +0.02 to +0.04 F1

================================================== SECTION 5 — OPTIONAL
/ RESEARCH LEVEL IMPROVEMENTS
==================================================

1.  Add More Derived Metadata Features
    -   Followers / Following ratio
    -   Tweet frequency patterns
    -   Activity time distributions
2.  Advanced Text Cleaning
    -   Emoji handling
    -   Spell normalization
    -   Noise reduction
3.  Threshold Tuning (Post Training)
    -   Improves Precision / Recall balance

================================================== SECTION 6 — REALISTIC
FUTURE PERFORMANCE TARGET
==================================================

Current: F1 ≈ 0.86

After High Priority Improvements: F1 ≈ 0.90 – 0.92

Research Level (Full Fine-Tuning + Advanced Training): F1 ≈ 0.93 – 0.94

================================================== SECTION 7 — SIMPLE
ROADMAP (LAPTOP FRIENDLY ORDER)
==================================================

STEP 1: Unfreeze last BERT layer only

STEP 2: Switch Adam → AdamW

STEP 3: Add Focal Loss

STEP 4: Add Stratified Sampling

STEP 5: Add LR Scheduler

================================================== SECTION 8 — MOST
IMPORTANT TAKEAWAY ==================================================

You already have: - Correct architecture - Stable training - Good
baseline F1

Main gap is: Training optimization sophistication, not model design.

================================================== END OF DOCUMENT
==================================================
